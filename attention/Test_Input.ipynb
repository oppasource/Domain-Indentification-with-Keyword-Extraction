{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import os\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "import model as m\n",
    "\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "\n",
    "\n",
    "\n",
    "############# Parameters\n",
    "path2embeddings = '../pretrained_embeds/glove.6B/'\n",
    "embedfile = 'glove.6B.50d'\n",
    "\n",
    "path2data = '../data/'\n",
    "datafile = 'news_articles.pkl'\n",
    "\n",
    "cat2id_file = 'category2id.pkl'\n",
    "\n",
    "model_saving_path = '../attention/'\n",
    "model_saving_file = 'attention_model.pt'\n",
    "\n",
    "article_max_len = 600\n",
    "top_k_keywords = 10\n",
    "\n",
    "embed_size = 50\n",
    "hidden_dim = 100\n",
    "n_classes = 7\n",
    "\n",
    "\n",
    "\n",
    "############# Loading Pretrained Glove Embeddings\n",
    "if os.path.isfile(path2embeddings + embedfile + '_w2v.txt'):\n",
    "    glove_model = KeyedVectors.load_word2vec_format(path2embeddings + embedfile + '_w2v.txt', binary=False)\n",
    "else:\n",
    "    glove2word2vec(glove_input_file=path2embeddings + embedfile + '.txt', word2vec_output_file=path2embeddings + embedfile + '_w2v.txt')\n",
    "    glove_model = KeyedVectors.load_word2vec_format(path2embeddings + embedfile + '_w2v.txt', binary=False)\n",
    "\n",
    "def get_embed(word):\n",
    "    # Case folding\n",
    "    word = word.lower()\n",
    "    try:\n",
    "        return (glove_model.get_vector(word))\n",
    "    except:\n",
    "        return (glove_model.get_vector('<unk>'))\n",
    "    \n",
    "    \n",
    "\n",
    "############## Categories to its id\n",
    "if os.path.exists(path2data + cat2id_file):\n",
    "    with open(path2data + cat2id_file, 'rb') as handle:\n",
    "        category2id = pickle.load(handle)\n",
    "# inverse the dict\n",
    "id2category = {v: k for k, v in category2id.items()}\n",
    "        \n",
    "        \n",
    "        \n",
    "############ Loading the model\n",
    "# Using gpu if available else cpu\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = m.atten_classifier(embed_size, hidden_dim, n_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.load_state_dict(torch.load(model_saving_path + model_saving_file))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(model_saving_path + model_saving_file, map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translateByTokens(text, source_lang='hi'):\n",
    "#     text = text.split()\n",
    "    \n",
    "    # pair of (src_token, dest_token)\n",
    "#     trans_text = [(w, translator.translate(w, src=source_lang).text) for w in text]\n",
    "    trans_text = []\n",
    "    for w in text:\n",
    "        try:\n",
    "            trans_text.append((w, translator.translate(w, src=source_lang).text))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # translated text\n",
    "    eng_text = \" \".join([elem[1] for elem in trans_text])\n",
    "    \n",
    "    # can return the cleaned the eng_text if desired\n",
    "    # eng_text = clean_text(eng_text)\n",
    "    return (trans_text, eng_text)\n",
    "\n",
    "def translate(text, source_lang='hi'):\n",
    "    eng_text = translator.translate(text, src=source_lang).text\n",
    "    return eng_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pred(article, is_hindi = True):\n",
    "    stop = stopwords.words('english') + list(string.punctuation)\n",
    "\n",
    "\n",
    "    if is_hindi:\n",
    "        article = translate(inp_article)\n",
    "\n",
    "    article = article.lower()\n",
    "    article = nltk.word_tokenize(article)\n",
    "    article_words = [i for i in article if i not in stop]\n",
    "    article = [get_embed(j) for j in article_words]\n",
    "    article = np.array(article[:article_max_len])\n",
    "\n",
    "    article_inp = torch.from_numpy(article).to(device)\n",
    "    out, alphas = model(article_inp)\n",
    "\n",
    "    ###### Getting the domain of news article\n",
    "    domain = id2category[int(torch.argmax(out).data.cpu().numpy())]\n",
    "\n",
    "    ####### Getting keywords with higest alpha weights\n",
    "    word2weights = {}\n",
    "    extracted_keywords = []\n",
    "    alpha_weights = alphas.data.cpu().numpy().reshape(-1).tolist()\n",
    "    for i in range(len(article_words)):\n",
    "        word2weights[article_words[i]] = alpha_weights[i]\n",
    "    word2weights = sorted(word2weights.items(), key=lambda kv: kv[1], reverse= True)\n",
    "    for j in range(top_k_keywords):\n",
    "        extracted_keywords.append(word2weights[j][0])\n",
    "\n",
    "    if is_hindi:\n",
    "        hindi_keywords = []\n",
    "        for e in extracted_keywords:\n",
    "            hindi_keyword = translator.translate(e, src='en', dest = 'hi').text\n",
    "            hindi_keywords = hindi_keywords + hindi_keyword.split()\n",
    "            \n",
    "        hindistop = stopwords.words('hindi') + list(string.punctuation)\n",
    "        find_hindi_keywords = [i for i in hindi_keywords if i not in hindistop]\n",
    "        find_hindi_keywords = [i for i in find_hindi_keywords if len(i)>2]\n",
    "        extracted_keywords = find_hindi_keywords\n",
    "\n",
    "    return domain, extracted_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the news article: Facebook, in a bid to restrict the spread of hate speech and misinformation on its platform, has banned US conspiracy theorist Alex Jones and other right far-right extremist personalities including Nation of Islam leader Louis Farrakhan, Paul Nehlen, Paul Joseph Watson, Milo Yiannopoulos and Laura Loomer from its platform. Apart from banning these leaders from Facebook, the social media giant has also banned them and their associated pages from Instagram, which in recent times had become the hotbed for extremist views and misinformation.  The strictest ban, as The Atlantic notes in its report, comes against Jones, whose Facebook profile had been suspended last July, and his InfoWars page. The Menlo Park headquartered social media giant has not only banned Jones profile and InfoWars from Facebook and from Instagram but the company has also decided to remove all the content from the two platforms that contain videos, articles and radio segments by the page. In addition to this Facebook will also remove any group, both on Facebook and on Instagram, that share content by InfoWars or events that promote any of the banned right-extremists figures.  \"We've always banned individuals or organizations that promote or engage in violence and hate, regardless of ideology. The process for evaluating potential violators is extensive and it is what led us to our decision to remove these accounts today,\" a Facebook spokesperson told The Atlantic in a statement.  Notably, while Facebook has banned these extremist figures and groups from sharing their content on its platforms, users can still praise these personalities and share content related to them on Facebook and Instagram provided it doesn't violate its terms and conditions.\n",
      "\n",
      "Identified Domain: technology\n",
      "\n",
      "Extracted Keywords: ['platform', 'facebook', 'sharing', 'users', 'platforms', 'videos', 'company', 'bid', 'instagram', 'giant']\n"
     ]
    }
   ],
   "source": [
    "is_hindi = False\n",
    "inp_article = input('Enter the news article: ')\n",
    "\n",
    "domain, keywords = make_pred(inp_article, is_hindi=is_hindi)\n",
    "\n",
    "print('\\nIdentified Domain: ' + str(domain) + '\\n')\n",
    "print('Extracted Keywords: ' + str(list(set(keywords))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
